---
title: "Statistical inference and experimental design in R"
author: "Pedro López-Escribano Zamora"
date: "30/12/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r , warning=FALSE, message=FALSE}
library(tidyverse)
library(infer)
library(pwr)
sessionInfo()
```

## Gender Discrimination

Data

```{r}
df <- data.frame(
    promote = c(rep("promoted", 350), rep("not_promoted", 130)),
    sex = c(rep("male", 180), rep("female",170), # promoted
            rep("male", 50), rep("female",80)))    # not_promoted

df %>% 
    count(sex, promote)

df %>% 
    group_by(sex) %>%
    summarize(promote_prop = mean(promote == "promoted"))
```

* The important stadistical question to ask after looking at the data is as follows: 

**Is it plausible to observe such a difference in proportions, in a scenario where men and women are equally to be promoted?**
    
### Gender Discrimination Hypothesis

* **H0:** Gender and promotion are unrelated variables.
* **H1:** Men are more likely to be promoted


### Randomizing gender discrimination 

```{r}
diff_orig <- df %>% 
    group_by(sex) %>%
    summarize(prop_prom = mean(promote == "promoted")) %>%
    summarize(stat = diff(prop_prom)) %>% 
    pull()

df_perm <- df %>%
    specify(promote ~ sex, success = "promoted") %>% 
    hypothesize(null = "independence") %>% 
    generate(reps = 1000, type = "permute") %>% 
    calculate(stat = "diff in props", order = c("male","female"))

ggplot(df_perm, aes(x = stat)) + 
    geom_histogram(binwidth = 0.03) + 
    geom_vline(xintercept = diff_orig, color = "red")

df_perm %>%
    visualize() +
        shade_pvalue(obs_stat = diff_orig, direction = "greater")

df_perm %>% 
    get_p_value(obs_stat = diff_orig, direction = "greater")
```

**In the population there is evidence that women are promote at diferent rate, but we cannot tell whether the difference is due to discrimination or something else.**


## Bootstrap

```{r}
all_polls <- readRDS("all_polls.rds")
glimpse(all_polls)
# Resampling from a sample

# P-hats from all polls
exp1_props <- all_polls %>% 
    group_by(poll) %>% 
    summarize(stat = mean(vote == "yes"), n = n())

# P-hats from one poll with bootstrap
exp2_props <- all_polls %>% 
    filter(poll == 1) %>% 
    specify(response = vote, success = "yes") %>% 
    generate(reps = 1000, type = "bootstrap") %>% 
    calculate(stat = "prop")

exp1_props %>% summarize(variability = sd(stat))

exp2_props %>% summarize(variability = sd(stat))
```

**The variability in the proportion of “successes” in a sample is approximately the same whether we sample from the population or resample from a sample.**

```{r}    
combined_exp <- bind_rows(exp1_props, exp2_props, .id = "experiment")    

ggplot(combined_exp, aes(stat, color = experiment)) + 
    geom_density(bw =0.1)
    
```

Note that the curves are quite similar in shape. The sampled values are centered around the true (typically unknown parameter) parameter (red); the resampled 
 values are centered around the estimate from the very first poll (blue).


### Bootstrap t-confindence interval

```{r}
one_poll <- all_polls %>% 
    filter(poll == 1) %>% 
    select(vote) # sample

one_poll_bootstrap <- one_poll %>% 
    specify(response = vote , success = "yes") %>% 
    generate(reps = 1000, type = "bootstrap") %>% 
    calculate(stat = "prop") 

p_hat <- one_poll %>% 
    summarize(stat = mean(vote == "yes")) %>% pull() # P-hat 
p_hat

ci <- one_poll_bootstrap %>% 
    summarize(lower = p_hat - (2* sd(stat)),
              upper = p_hat + (2* sd(stat)))
ci

one_poll_bootstrap %>% 
    visualise() + 
        shade_ci(ci)
```

**Indeed, with the sample at hand, the confidence interval of plausible values does contain the true population parameter of 0.7.**

## About power and sample size 

* **Power:** Probability that the test correctly rejects the null hypothesis, when the alternative hypothesis is true

* **Effect size:** Standardized measure of the difference yo're trying to detect

* **Sample size:**  How many experimental units you need to survey to detect the desired
# difference at the desired power

```{r}
pwr.anova.test(k = 3, # groups
               n = 20, # N
               f = 0.2, # size effect  
               sig.level = 0.05,
               power =  NULL)  # set NULL to calculate
```

**In this case power is low, so probaly we can't detect a effect size so small with that number of people per group.**


We want to know what N needs to detect size effect of 0.25, with sig.level = 0.05, from one sample, alternative hypothesy "greater" and power 80%.

```{r}
pwr.t.test(n = NULL, 
           d = 0.25,
           sig.level = 0.05,
           type = "one.sample", alternative = "greater",
           power = 0.8)
```

### Lending Club Data

```{R}
lendingclub <- read.csv("https://assets.datacamp.com/production/repositories/1793/datasets/e14dbe91a0840393e86e4fb9a7ec1b958842ae39/lendclub.csv",
               stringsAsFactors = T)
lendingclub <- as_tibble(lendingclub)
str(lendingclub)
glimpse(lendingclub)

lendingclub %>% 
    summarize(
        median_loan_amnt = median(loan_amnt),
        mean_int_rate = mean(int_rate),
        mean_annual_inc = mean(annual_inc))

ggplot(lendingclub, aes(x = purpose)) + 
    geom_bar() + coord_flip()

# Recode purpose var

lendingclub$purpose_recode <- lendingclub$purpose %>% recode( 
    "credit_card" = "debt_related", 
    "debt_consolidation" = "debt_related",
    "medical" = "debt_related",
    "car" = "big_purchase", 
    "major_purchase" = "big_purchase", 
    "vacation" = "big_purchase",
    "moving" = "life_change", 
    "small_business" = "life_change", 
    "wedding" = "life_change",
    "house" = "home_related", 
    "home_improvement" = "home_related")

ggplot(lendingclub, aes(x = purpose_recode)) + 
    geom_bar() + coord_flip()

```

## ANOVA

Remember that for an ANOVA test, the null hypothesis will be that all of the mean funded amounts are equal across the levels of purpose_recode. The alternative hypothesis is that at least one level of purpose_recode has a different mean.

 We will not be sure which, however, without some post hoc analysis, so it will be helpful to know how ANOVA results get stored as an object in R.

```{r}
lendingclub_lm <- lm(funded_amnt ~ purpose_recode , data = lendingclub)

summary(lendingclub_lm)

anova(lendingclub_lm)
```


**Based one the very low p-value, purpose recode anova results indicate that there is evidence to support the hypothesis that the mean load amounts are different for at least one combination of purpose_recode's levels.**

### Which loan purpose mean is different?

The result of that ANOVA test was statistically significant with a very low p-value. This means we can reject the null hypothesis and accept the alternative hypothesis that at least one mean was different. But which one? We should use Tukey's HSD test, which stands for Honest Significant Difference.

```{r}
purpose_anova <- aov(funded_amnt ~ purpose_recode , data = lendingclub) #another way to do ANOVA

summary(purpose_anova)

hsd_purpose<- TukeyHSD(purpose_anova, "purpose_recode", conf.level = 0.95)

broom::tidy(hsd_purpose) %>% arrange(adj.p.value)
```


Looking at the p-values for each comparison of the levels of purpose_recode, we can see that only a few of the mean differences are statistically significant, for example the differences in the means for the **debt_related** and **big_purchase** loan amounts. In this case, these tiny p-values are most likely to be due to large sample size, and further tests would be required to determine what's actually significant in the case of loans (known as the practical significance.)

* We can examine more than one explanatory factor in a **multiple factor experiment**. Let's see how p-values changes after control emp_length.

```{r}
purpose_emp_anova<- aov(funded_amnt ~ purpose_recode + emp_length, data = lendingclub)

summary(purpose_emp_anova)

hsd_purpose_emp<- TukeyHSD(purpose_emp_anova, "purpose_recode", conf.level = 0.95)

broom::tidy(hsd_purpose_emp) %>% arrange(adj.p.value)
```

### Interest rate and grade

Let's do some EDA with our experiment in mind. Lending Club has now asked you, their data scientist, to examine what effect their Lending Club-assigned loan grade variable has on the interest rate, int_rate. They're interested to see if the grade they assign the applicant during the process of applying for the loan affects the interest rate ultimately assigned to the applicant during the repayment process.

```{r}
summary(lendingclub$int_rate)

lendingclub %>% 
    group_by(grade) %>% 
        summarize(median_int_rate = median(int_rate),
                  mean_int_rate = mean(int_rate),
                  var_int_rate = var(int_rate))

ggplot(lendingclub, aes(x = grade, y = int_rate)) + 
    geom_boxplot() + 
        labs(title = "Int Rate by Grade")

grade_anova <- aov(int_rate ~ grade, data = lendingclub)

summary(grade_anova)
```

The numeric summary and the boxplot that grade seems to heavily influence interest rate. Therefore, the linear model results indicating that int_rate is significantly different by grade are unsurprising.


### Post-modeling validation plots + variance

Another assumption of ANOVA and linear modeling is homogeneity of variance. Homogeneity means "same", and here that would mean that the variance of int_rate is the same for each level of grade. We can test for homogeneity of variances using **bartlett.test()**, which takes a formula and a dataset as inputs.

```{r}
par(mfrow = c(2,2))
plot(grade_anova)

bartlett.test(int_rate ~ grade, data = lendingclub)
```

The residuals on this model are okay, though the residuals on G have a much smaller range than any other level of grade (the dots are far less spread out.) The Q-Q plot, however, shows that the residuals are fairly normal. However, **given the highly significant p-value from Bartlett's test, the assumption of homogeneity of variances is violated**, which is one of the assumptions of an ANOVA model. Therefore, ANOVA might not be the best choice for this experiment. It happens!

* **Alternative**

One non-parametric alternative to ANOVA is the **Kruskal-Wallis rank sum test**. For those with some statistics knowledge, it is an extension of the Mann-Whitney U test for when there are more than two groups, like with our grade variable. For us, the null hypothesis for this test would be that all of the int_rates have the same ranking by grade.

```{r}
kruskal.test(int_rate ~ grade, data = lendingclub)
```

**The low p-value indicates that based on this test, we can be confident in our result, which we found across this experiment, that int_rate varies by grade.**

## A/B TEST

### Sample size for A/B test

We know now that we need to analyze our A/B test results with a t-test after we've collected data. We have two pretty important questions we need to answer before we do that: **What's the effect size and what's the sample size required for this test?** In this case, effect size was given to us. Lending Club is looking to detect the relatively small effect size of 0.2.

```{r}
pwr.t.test(n = NULL, d = 0.2, sig.level = 0.05, power = 0.8,
           type = "two.sample", alternative = "two.sided")
```

### Design of A/B Test

When applicants were using the Lending Club website, they were randomly assigned to two groups, A or B, where A was shown a mint green website header and B was shown a light blue website header.

**Lending Club was interested to see if website header color choice influenced loan_amnt, the amount an applicant asked to borrow.**


We know the sample size required, and we allowed the experiment to run long enough to get at least 400 people in each group, we can analyze our A/B test.

```{r}

group <- c( rep("A",500), rep("B", 500))

lendingclub_ab <- cbind(lendingclub[1:1000,], group) # New dataset with group column.

ggplot(lendingclub_ab, aes(x = group, y = loan_amnt)) + 
    geom_boxplot()      # Plop A/B results

t.test(loan_amnt ~ group, data = lendingclub_ab, alternative = "two.sided")
```

**By looking at both the boxplot and the results of the t-test, it seems that there is no compelling evidence to support the hypothesis that there is a difference the two A/B test groups' mean loan_amnt, a result which you would use to help make data-driven decisions at Lending Club.**

**Note:** I must say that analyzing the results of an A/B test is easy, the real challenge is the design of the experiment, making a good random and/or stratified sample, taking into account confounding variables and side effects is the most critical part of this type of task.

### Multivariate test

A Lending Club multivariate test can combine all of the explanatory variables

```{r}
lendingclub_multi <- lm(loan_amnt ~ group + grade + verification_status, data = lendingclub_ab)

broom::tidy(lendingclub_multi)
```

**From the results, verification status and having an F grade are the factors in this model that have a significant effect on loan amount.**

## Inference for Categorical Data

### Data

* General Social Survey
    * The GSS gathers data on contemporary American society in order to monitor and explain trends and constants in attitudes, behaviors, and attributes. ... 
    * The GSS contains a standard core of demographic, behavioral, and attitudinal questions, plus topics of special interest.

```{r}
gss <- get(load("gss.RData"))
glimpse(gss)
summary(gss)
sapply(gss, n_distinct)
naniar::miss_var_summary(gss) %>% head(20) 
gss2016 <- gss %>% filter(year == 2016) %>% drop_na(postlife, cappun, natspac, natarms,happy)
```

* One of the questions that was asked of respondents was: 
    **Do you believe there is a life after death?**

Let's see how your sample of Americans responded to this question in 2016.

```{r}
ggplot(gss2016, aes(x = postlife)) +
    geom_bar()

p_hat <- gss2016 %>%  
            summarize(prop_yes = mean(postlife == "YES")) %>% pull()
p_hat
```

### Generating from H0

Imagine that when reading the newspaper, you come across someone who makes the following claim: "3/4 of all Americans believe in life after death".
**This can be interpreted as a point null hypothesis that the population proportion has a value of 0.75.**

```{r}
null <- gss2016 %>% 
            specify(response = postlife, success = "YES") %>% 
            hypothesize(null = "point", p = 0.75) %>% 
            generate(reps = 1000, type = "simulate") %>% 
            calculate(stat = "prop")

ggplot(null, aes(stat)) +
    geom_density() +
    geom_vline(xintercept = p_hat, color = "red")

null %>%
    summarize(two_tailed_pval = mean(stat >= p_hat)*2) %>%
        pull(two_tailed_pval)
```
**The p-value < alpha, thus the data is inconsistent with the null hypothesis so I reject it as a reasonable explanation.**

### Death penalty and sex

While you're on the topic of death and the afterlife, take a look at another question from the GSS:

Do you favor or oppose the death penalty for people convicted of murder?

**The objective here is to explore if opinions on capital punishment (cappun) diverged between men and women in the gss2016 data.**

```{r}
ggplot(gss2016, aes(x = cappun, fill = sex)) +
    geom_bar(position = "fill")

gss2016 %>% group_by(sex) %>% 
    summarize(prop_FAVOR = mean(cappun == "FAVOR"))

p_hats <- gss2016 %>% group_by(sex) %>% 
            summarize(prop_FAVOR = mean(cappun == "FAVOR")) %>% pull()

d_hat <- diff(p_hats)
d_hat
```
**Note:** R will do operations like this alphabetically, so -0.08216367 is the proportion of (f)emales that favor minus the proportion of (m)ales.

We learned that about 58% of women favor the death penalty while about 66% of men do, a difference of about 8 percentage points. That seems like a large difference, **but what if it's just due to chance and in fact there is no relationship between sex and support for the death penalty?**

```{r}
null<- gss2016 %>% 
            specify(cappun ~ sex, success = "FAVOR") %>% 
            hypothesize(null = "independence") %>% 
            generate(reps = 1000, type = "permute") %>%
            calculate(stat = "diff in props", order = c("FEMALE", "MALE"))

ggplot(null, aes(stat)) +
    geom_density() +
    geom_vline(xintercept = d_hat, color = "red")

null %>%
    summarize(two_tailed_pval = mean(stat >= p_hat)*2) %>%
    pull(two_tailed_pval)
```
**The data are strong evidence that there is an association. Means that there is a difference between men and women in the opinion about the capinal punishment**

### Confidence Interval

```{r}
boot <- gss2016 %>%
    specify(cappun ~ sex, success = "FAVOR") %>%
    generate(reps = 1000, type = "bootstrap") %>%
    calculate(stat = "diff in props", order = c("FEMALE", "MALE"))

SE <- boot %>%
    summarize(se = sd(stat)) %>%
    pull()

ci <- c(d_hat - (2*SE), d_hat + (2*SE))
ci
boot %>% visualise() + 
    shade_ci(ci)
```

**CI not included the value of zero, indicating that it is a not plausible value. Leading the same conclusion as the hypothesis test.**

## Contingency Tables and Chip-square.

The Chi-Square test of independence is used to determine if there is a significant relationship between two nominal (categorical) variables.  The frequency of each category for one nominal variable is compared across the categories of the second nominal variable.  The data can be displayed in a contingency table where each row represents a category for one variable and each column represents a category for the other variable.  For example, say a researcher wants to examine the relationship between gender (male vs. female) and empathy (high vs. low).  The chi-square test of independence can be used to examine this relationship.  The null hypothesis for this test is that there is no relationship between gender and empathy.  The alternative hypothesis is that there is a relationship between gender and empathy (e.g. there are more high-empathy females than high-empathy males).

More info: https://www.statisticssolutions.com/non-parametric-analysis-chi-square/


### Politics and Space.

What about the relationship between political party and another spending priority: space exploration?

```{r}
gss2016$party <- recode(gss2016$partyid,
       "STRONG DEMOCRAT" = "Dem",
       "NOT STR DEMOCRAT" = "Dem",
       "IND,NEAR DEM" = "Ind",
       "INDEPENDENT" = "Ind",
       "IND,NEAR REP" = "Ind",
       "NOT STR REPUBLICAN" = "Rep",
       "STRONG REPUBLICAN" = "Rep",
       "OTHER PARTY" = "Oth")

gss_party <- gss2016 %>% filter(party != "Oth")
gss_party$party <- droplevels(gss_party$party, "Oth")

ggplot(gss_party, aes(party, fill = natspac)) + 
    geom_bar() + 
        labs(
            title = "Visual representation of a contingency table")

# natspac ~ party
chi_obs_spac <- gss_party %>% chisq_stat(natspac ~ party)

null_spac <- gss_party %>%
    specify(natspac ~ party) %>%
    hypothesize(null = "independence") %>%
    generate(reps = 1000, type = "permute") %>%
    calculate(stat = "Chisq")

ggplot(null_spac, aes(x = stat)) +
    geom_density() +
    geom_vline(xintercept = chi_obs_spac, color = "red")

null_spac %>% 
    summarize(p_val = mean(stat >= chi_obs_spac))

# natarms ~ party

gss_party %>% select(natarms, party) %>% table()

chi_obs_arms <- gss_party %>% chisq_stat(natarms ~ party)

null_arms <- gss_party %>%
    specify(natarms ~ party) %>%
    hypothesize(null = "independence") %>%
    generate(reps = 1000, type = "permute") %>%
    calculate(stat = "Chisq")

ggplot(null_arms, aes(x = stat)) +
    geom_density() +
    geom_vline(xintercept = chi_obs_arms, color = "red")

null_arms %>% 
    summarize(p_val = mean(stat >= chi_obs_arms))
```
**The data set is consistent with the hypothesis that there is no relationship between political party and space exploration spending, but does suggestion a relationship between party and spending on the military.**


### Alternate method: the chi-squared distribution

* Approximation distributions: chi-squared
    * Shape is determiden by degrees of freedom 
    * df = (nrows- 1) x (ncols - 1)
* Becomes a good approximation when: 
    * Expected_count >= 5 (in each cell)
    * df >= 2
    * Large sample size 
    
```{r}
# Region ~ Happy

gss2016_RH <- gss2016 %>% select(region, happy)
gss2016_RH$happy <- recode(gss2016_RH$happy,
                           "VERY HAPPY" = "HAPPY",
                           "PRETTY HAPPY" = "HAPPY",
                           "NOT TOO HAPPY" = "UNHAPPY") 

table(gss2016_RH)

degrees_of_freedom <- (nrow(table(gss2016_RH)) - 1) * (ncol(table(gss2016_RH)) - 1) 
degrees_of_freedom


ggplot(gss2016_RH, aes(y = region, fill = happy)) + 
    geom_bar(position = "fill") 
chi_obs <- gss2016_RH %>% chisq_stat(region ~ happy)

null <- gss2016_RH %>%
    specify(happy ~ region, success = "HAPPY") %>%
    hypothesize(null = "independence") %>%
    generate(reps = 500, type = "permute") %>%
    calculate(stat = "Chisq")

ggplot(null, aes(stat)) + 
    geom_density(lwd = 2) +
    geom_vline(xintercept = chi_obs, color = "red") + 
    stat_function(fun = dchisq, args = list(df = degrees_of_freedom), color = "blue", lwd = 2)

# Calculate computational pval
null %>%
    summarize(pval = mean(stat >= chi_obs))

# Calculate approximate pval
pchisq(chi_obs, df = degrees_of_freedom, lower.tail = FALSE)
```

## Bonus

* **How likely is it to find at least 2 people with the same birthday in a group?**

To see how the odds increase as the size of people increases.

```{r}
people <- seq(2,58)
B <- 10000
probs <-vector(mode = "numeric")
for (n in people){
    simulate <- replicate(B, {
        sample <- sample(seq(1:365), n, replace = TRUE)
        same_day <- any(duplicated(sample))
        
    })
    probs <- append(probs, mean(simulate)) 
}
bdays_probs <- data.frame(people,probs) 
bdays_probs

ggplot(bdays_probs, aes(x = people, y = probs)) + 
    geom_line() + 
        geom_point()
```

To see how long it takes for the probabilities to stabilize (Law of Large Numbers)

```{r}
people <- 25
n_simulations <- 10^seq(1, 5, len = 100)
probs <-vector(mode = "numeric")

for (b in n_simulations){
    simulate <- replicate(b, {
        sample <- sample(seq(1:365), people, replace = TRUE)
        same_day <- any(duplicated(sample))
        
    })
    probs <- append(probs, mean(simulate)) 
}

bdays_probs <- data.frame(n_simulations,probs) 
bdays_probs

ggplot(bdays_probs, aes(x = n_simulations, y = probs)) + 
    geom_line() 

ggplot(bdays_probs, aes(x = log(n_simulations), y = probs)) + 
    geom_line() 

```

I find it extremely useful to practice this type of thing, I encourage you to practice this type of scenario in R from 0.

**Thaks for you interest**




